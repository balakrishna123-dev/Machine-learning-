{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3f286b-faa6-49a1-b36e-ec46890e569e",
   "metadata": {},
   "source": [
    " # What is Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56223025-c6aa-47ee-86a2-643195d65d5f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Logistic Regression is a supervised machine learning algorithm used for classification problems, mainly binary classification.\n",
    "\n",
    "Output is categorical, not numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a6637-7639-4df2-80b1-29aa3a62ddbb",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "\n",
    "Loan approval::\tYes / No\n",
    "Disease detection::\tPositive / Negative\n",
    "Customer churn\t::Leave / Stay\n",
    "Fraud detection\t::Fraud / Not Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e7df4-a948-4773-94e8-5f3a434738b3",
   "metadata": {},
   "source": [
    " # Why Not Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a1c69-34b1-44fe-8294-7860687d6458",
   "metadata": {},
   "source": [
    "\n",
    "Linear Regression Output:\n",
    "\n",
    " Can be -2, 1.8, 3.4\n",
    "\n",
    "Logistic Regression Output:\n",
    "\n",
    " Always between 0 and 1 (probability)\n",
    "\n",
    " Classification needs probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc3608-c513-4028-8bc7-654c60e70378",
   "metadata": {},
   "source": [
    "# Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046cda2-3ff9-426c-b76c-e37f2327f877",
   "metadata": {},
   "source": [
    "The sigmoid function is a mathematical function used in logistic regression to convert any real number into a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8662b1f-65f7-4067-9a40-8315d3a5bb5b",
   "metadata": {},
   "source": [
    " formula :  σ(y)=1+e−y1​\n",
    "Where:\n",
    "\n",
    "z = weighted sum of inputs \n",
    "\n",
    "e = Euler’s number (≈ 2.718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2adf0a9e-4cea-4372-ba41-4a23302bc316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.9933071490757153\n",
      "0.0066928509242848554\n"
     ]
    }
   ],
   "source": [
    "# Why Sigmoid?\n",
    "\n",
    "# Converts any value to 0–1\n",
    "\n",
    "# Looks like an S-curve\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "print(sigmoid(0))   # 0.5\n",
    "print(sigmoid(5))   # close to 1\n",
    "print(sigmoid(-5))  # close to 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54b90e-6741-4162-8a69-43c13e728f69",
   "metadata": {},
   "source": [
    "# Soft Max Function:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b175a3-1052-49ae-92f6-346387e5f25d",
   "metadata": {},
   "source": [
    "The softmax function is used in multi-class classification problems to convert raw model outputs (scores/logits) into probabilities that sum to 1.\n",
    "\n",
    " It is the multi-class extension of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8949ea06-821f-466d-a2ae-6f94aed8c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why Softmax is Needed?\n",
    "\n",
    "# Model outputs raw scores (can be any number)\n",
    "\n",
    "# We need:\n",
    "\n",
    "# Probabilities\n",
    "\n",
    "# One class to be chosen\n",
    "\n",
    "# Total probability = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1de5eaf-9215-4c02-816f-f8b9c0b686ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65900114, 0.24243297, 0.09856589])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Examples:\n",
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "scores = np.array([2.0, 1.0, 0.1])\n",
    "softmax(scores)\n",
    "\n",
    "# which  class has the highest label it will take it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9e325a9-3c08-42fd-8ba5-a45d375e99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Evalution Metrics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179306a6-47bf-491b-9274-cdf42d015220",
   "metadata": {},
   "source": [
    " # Confusion Matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3826d1-aa03-405d-bd19-6aed2007f7e3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A confusion matrix is a table used to see how well a classification model is performing.\n",
    "It shows the number of correct predictions (true positives and true negatives) and wrong predictions (false positives and false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf2cac-7dbf-4cf8-b636-7254f10a3f09",
   "metadata": {},
   "source": [
    "Actual\tPredicted Positive\tPredicted Negative\n",
    "Positive\tTP ✅\t        FN ❌\n",
    "Negative\tFP ❌\t        TN ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf3aec-e1a6-45df-a3c4-5edf22557804",
   "metadata": {},
   "source": [
    "Where the terms have the meaning:\n",
    "\n",
    "__True Positive(TP):__ A result that was predicted as positive by the classification model and also is positive\n",
    "\n",
    "__True Negative(TN):__ A result that was predicted as negative by the classification model and also is negative\n",
    "\n",
    "__False Positive(FP):__ A result that was predicted as positive by the classification model but actually is negative\n",
    "\n",
    "__False Negative(FN):__ A result that was predicted as negative by the classification model but actually is positive.\n",
    "\n",
    "The Credibility of the model is based on how many correct predictions did the model do.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbf8f2-1303-4c47-b0aa-ff9280c26625",
   "metadata": {},
   "source": [
    " Example:\n",
    "Imagine we are building a model to detect COVID-19 from test results:\n",
    "\n",
    "Positive: The person actually has COVID-19.\n",
    "\n",
    "Negative: The person does NOT have COVID-19.\n",
    "\n",
    "Now, predictions by our model:\n",
    "\n",
    " 1:True Positive (TP):\n",
    "\n",
    "Model predicts COVID-19 = Positive\n",
    "\n",
    "Person actually has COVID-19\n",
    "\n",
    "✅ Correct detection\n",
    "\n",
    "2:True Negative (TN):\n",
    "\n",
    "Model predicts COVID-19 = Negative\n",
    "\n",
    "Person actually does NOT have COVID-19\n",
    "\n",
    "✅ Correct detection\n",
    "\n",
    "3:False Positive (FP):\n",
    "\n",
    "Model predicts COVID-19 = Positive\n",
    "\n",
    "Person actually does NOT have COVID-19\n",
    "\n",
    "❌ Wrong! The model says they are sick but they are healthy\n",
    "\n",
    "Also called a Type I Error\n",
    "\n",
    "4:False Negative (FN):\n",
    "\n",
    "Model predicts COVID-19 = Negative\n",
    "\n",
    "Person actually has COVID-19\n",
    "\n",
    "❌ Wrong! The model misses a sick person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80bd828-e6ee-485a-80b7-99f96ad24328",
   "metadata": {},
   "source": [
    "\n",
    " # Accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bba6f6-e663-4a7c-b051-a617104d63b7",
   "metadata": {},
   "source": [
    "The fraction of total correct predictions out of all predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067d7e8-78d9-411e-8369-5fc3a1ff73ea",
   "metadata": {},
   "source": [
    " formula:\n",
    "Accuracy=(TP+TN)/(FP+FN+TP+TN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d681e-858f-42b0-94f7-b46590b91897",
   "metadata": {},
   "source": [
    " Example:\n",
    "\n",
    "Suppose 100 COVID tests:\n",
    "\n",
    "TP = 40, TN = 50, FP = 5, FN = 5\n",
    "\n",
    "Accuracy=(40+50)/(40+50+5+5)= 0.9 ==90%\n",
    "\n",
    "Accuracy is high when both positive and negative predictions are mostly correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17118185-46a5-4344-a161-10750a4fab7f",
   "metadata": {},
   "source": [
    " # Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2aab49-7cdd-40a6-8439-fe29507045af",
   "metadata": {},
   "source": [
    "\n",
    "Of all the cases predicted as positive, how many were actually positive\n",
    "\n",
    "Formula:\n",
    "\n",
    "Precision= TP/(TP+FP)= 40/(40+5)=88.8%\n",
    "\n",
    "Precision tells us how accurate our positive predictions are.\n",
    "\n",
    " Precision Minimize false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e92075-b36e-4065-b3ac-6bb99033d4c6",
   "metadata": {},
   "source": [
    " # Recall (Sensitivity / True Positive Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353aa101-1cd9-4d7a-a9c1-91c23f2af938",
   "metadata": {},
   "source": [
    "Of all actual positive cases, how many did we correctly predict as positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cec45b-6700-4594-b482-147458f06eb0",
   "metadata": {},
   "source": [
    " formula:\n",
    "Recall=TP+(FN+TP)\n",
    "\n",
    "TP = 40, FN = 5\n",
    "=40/(40+5)=88.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c2207-94a8-4f80-a534-c6c0f191a071",
   "metadata": {},
   "source": [
    " #  F1 Score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99bab3-6b79-4985-bd08-40d8865c9e52",
   "metadata": {},
   "source": [
    "F1 Score is the harmonic mean of precision and recall:\n",
    "\n",
    "\n",
    "F1=2×((Precision+Recall)/(Precision×Recall))\n",
    "\n",
    " form the example , when precision and recall are equal, the F1 Score = Precision = Recall.\n",
    "\n",
    " Note;the model had a very high Accuracy but performed poorly in terms of Precision and Recall. So, necessarily _Accuracy_ is not the metric to use for evaluating the model in this case. we are using f1 score as metrics?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b93ea-f002-4b86-9c6a-00ca0cf5234d",
   "metadata": {},
   "source": [
    " # Specificity (True Negative Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2387bee-d07e-422c-91ad-b9610608b629",
   "metadata": {},
   "source": [
    "Of all actual negative cases, how many did we correctly predict as negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7699f-13a8-48c1-b3b1-9ac02ecb3081",
   "metadata": {},
   "source": [
    " formula:\n",
    "Specificity=TN/(FP+TN)\n",
    "TN = 50, FP = 5\n",
    "S=50/55=0.909=90.9%\n",
    "\n",
    "Specificity measures how well the model identifies negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf52ba9-6f08-4e92-ac92-a8a8afa920fb",
   "metadata": {},
   "source": [
    " # ROC Curve (Receiver Operating Characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3bcf00-8a0a-41dc-bdf6-df1cea9024b4",
   "metadata": {},
   "source": [
    "the classification algorithms work on the concept of probability of occurrence of the possible outcomes. A probability value lies between 0 and 1. Zero means that there is no probability of occurrence and one means that the occurrence is certain.\n",
    "\n",
    "But while working with real-time data, it has been observed that we seldom get a perfect 0 or 1 value. Instead of that, we get different decimal values lying between 0 and 1. Now the question is if we are not getting binary probability values how are we actually determining the class in our classification problem?\n",
    "\n",
    "There comes the concept of Threshold. A threshold is set, any probability value below the threshold is a negative outcome, and anything more than the threshold is a favourable or the positive outcome. For Example, if the threshold is 0.5, any probability value below 0.5 means a negative or an unfavourable outcome and any value above 0.5 indicates a positive or favourable outcome. \n",
    "\n",
    "Now, the question is, what should be an ideal threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc6166-910e-4572-bc6b-ab8e6d3dbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a graph that shows the performance of a classification model at all classification thresholds.\n",
    "\n",
    "# X-axis: False Positive Rate (FPR) = FP / (FP + TN)\n",
    "\n",
    "# Y-axis: True Positive Rate (TPR) / Recall = TP / (TP + FN)\n",
    "\n",
    "# Each point on the ROC curve corresponds to a different threshold for predicting positive vs. negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541c850-c7e9-402e-a95c-5bd6429d5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    " Roc and aoc is used for binary classifications?\n",
    " by default  any thing greater than 0.5 thersold  it considers as 1\n",
    " any  thing less than 0.5 it conserd as 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bac498-afc8-4ebe-87c8-c1dabe3dc8e6",
   "metadata": {},
   "source": [
    " # AUC (Area Under the ROC Curve)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da5275c5-f7b6-4622-b3eb-066b331594bd",
   "metadata": {},
   "source": [
    "AUC stands for Area Under the ROC Curve.\n",
    "\n",
    "# ROC curve plots TPR (Recall) vs FPR at different thresholds.\n",
    "\n",
    "# AUC tells you the overall ability of your model to discriminate between positive and negative classes.\n",
    "\n",
    "# Range: 0 to 1\n",
    "\n",
    "# AUC = 1 →   better model \n",
    "\n",
    "# AUC = 0.5 → model is useless\n",
    "\n",
    "# AUC < 0.5 → Worse than random (model predicting wrong way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aebb7a-0e0c-4087-ae33-a110bc108444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
